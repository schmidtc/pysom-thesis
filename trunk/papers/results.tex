Before appling the diagnostics we must first train a series of SOMs.  Using the
ten synthetic datasets we train ten SOMs for each topology.  We find that the
mean internal heterogeneity remains fairly stable, suggesting that the results
of each simulation can be combined within a given topology.  For the rectangular
topology, we now have forty neurons with a degree of two for which an internal
heterogeneity can be calculated. It should be noted that we can only measure the
internal heterogeneity when a neuron captures two or more observations from the
training data.

For each topology, we calculated the internal heterogeneity and degree of all
the neurons.  We then grouped the internal heterogeneity measures by degree.  We
found that the means of these samples respond as expected for the rectangular
and hexagonal, or ``flat,''  topologies, but not in the spherical topologies.
This may suggest that the spherical and geodesic topologies are effectively
overcoming the edge problem.  The variance of the samples, however, did not
respond as expected.  In the flat topologies, the variance increased as the
degree increased, possibly due to the large difference in sample size.  To
verify these conclusions we formally test for differences in means and variance
between the samples using random labeling \cite{siss2004}.

In the rectangular and hexagonal topologies, we observe that all sample means
are significantly different.  We also observed differences in the variance of
the samples, except the case of degree size four and five in the hexagonal
topology, where they were not significantly different. No variances in the
spherical and geodesic topologies differed significantly.  In those topologies,
the only significant difference in means was between the sample with degree size
six and seven in the spherical topology.

Next we group the neurons of our trained SOMs based on their topology,
collapsing the neurons of the forty SOMs into four groups.  This results in one
sample for each of the four topologies.  We test for a difference in mean and
variance between each sample. No significant differences were found in the
variances.  The rectangular topology has the highest mean internal heterogeneity
and is the least regular as measured by closeness centrality. The geodesic and
spherical topologies are the most regular and have the lowest internal
heterogeneity with little difference between them.  This suggests that even
though the spherical topology is more irregular than the geodesic topology,
similar levels of quality may be achieved. 

\begin{figure}[ht]
\centering
\caption{Internal heterogeneity mapping}
%\subfigure[Rectangular Topology]{
%  \label{cluster:rook}
%  \includegraphics[width=0.7\linewidth]{rook_clusters.png}
%}
\subfigure[Hexagonal Topology]{
  \label{cluster:hex}
  \includegraphics[width=0.85\linewidth]{hex_clusters_no_border.png}
}
%\subfigure[Spherical Topology]{
%  \label{cluster:graph}
%  \includegraphics[width=0.7\linewidth]{sphere_clusters.png}
%}
\subfigure[Geodesic Sphere Topology]{
  \label{cluster:geodesic}
  \includegraphics[width=0.85\linewidth]{geodesic_clusters_no_border.png}
}
\label{cluster}
\end{figure}

The internal heterogeneity for the hexagon and geodesic topologies is visualized
in figure \ref{cluster}.  Neurons are represented as their voronoi regions.
Darker neurons had higher internal heterogeneity.  We also compute the cluster
each neuron represents by looking at the observations mapped to it.  Knowing
which cluster each observation belongs to allows us to see where the clusters
are being mapped to on the trained SOM. For a given neuron we classify it by
the cluster it captured most frequently.

It is interesting to note that the edges of the cluster exhibit higher internal
heterogeneity.  This is somewhat intuitive, as our clusters are normally
distributed; the majority of our observations will fall well within the regions
observed in figure \ref{cluster}. Each cluster's outliers will be pushed toward
the \emph{edges} of these regions. An observation that is on the edge of a
cluster in the original input-space is further away from the other observations
in the input-space.  Therefore, one would expect that observation to also be
near the edge of a cluster in the SOM space.  In order to represent a three
dimensional cluster in two dimensions, the SOM must compress the edges of the
clusters more than their centers.  This explains the higher internal
heterogeneity near the edges of the clusters.


%\subsection{The Utah-Hawaii Problem}
%\label{states}
 
%In a trained SOM, an earlier observation was made that Utah and Hawaii were similar because they both fell on the same neuron. This raised the question of whether Utah and Hawaii were in fact similar, or if this relationship in the SOM was merely caused by an edge effect.  The answer turns out to be slightly more complicated.  Utah and Hawaii are in fact each other's nearest neighbor, as defined by the census variables used by \cite{skupin08}.  However, as shown in Figure \ref{som:states} and explained in section \ref{bg:edge}, both of these observations can be considered outliers.  Neither fits the trained SOM well and both are far from the mean of all the states.

%In Figure \ref{sphere:states} we present the same data trained on a 400 neuron spherical SOM.  The topology for this SOM is based on \cite{Rakhmanov94}.  We find, not surprisingly, that Utah and Hawaii are separated in this SOM.  This implies that edge effects indeed played a large role in the placement of these two observations in the hexagonal SOM.  As suggested in section \ref{bg:sphere}, we also find these and other outlying observations occupy more space in the trained spherical SOM than in the hexagonal SOM.  States closer to the mean such as Ohio, Missouri and Kentucky are distinctly more clustered in the spherical SOM than in the hexagonal SOM.  It is interesting to note that the orientation between these three states remained largely unchanged between the two maps.
