\chapter{RESULTS AND DISCUSSION}
This chapter presents the results from our diagnostics.  Section \ref{rtrain}
reviews the training process.  Section \ref{rdq1} discusses the application of
the first diagnostic, which looks at how internal heterogeneity changes with a
neuron's first order neighborhood size, or degree.  In section \ref{rdq2} the
second diagnostic, which compares the mean internal heterogeneity across
topologies, is applied.  Section \ref{rdq3} applies the third
diagnostic, which visualizes the internal heterogeneity of each SOM. Finally,
section \ref{states} revisits the Utah-Hawaii example.

%The first diagnostic yields a set of results for each topology tested.  These results will be analyzed in order to address the first research question of this thesis.  The second diagnostic combines the results from each topology in order to address the second research question.  The final diagnostic will return a visualization for each topology tested.  The usefulness of these visualizations is a research question in itself.  The expectation is that the visualizations will show patterns of internal variance related to irregularities in the network topology.
\section{Training}
\label{rtrain}
%\section{Data}
Before appling the diagnostics we must first train a series of SOMs.  The
training is acomplished using our graph based implementation of SOM, PySOM.
Using the ten synthetic datasets we train ten SOMs for each topology.  The
mean internal heterogeneity of these SOMs is summarized in Table
\ref{ivtable3}.  We find that the mean internal heterogeneity remains fairly
stable, suggesting that the results of each simulation can be combined within
a given topology.  For the rectangular topology, we now have forty neurons
with a degree of two for which an internal heterogeneity can be calculated. It
should be noted that we can only measure the internal heterogeneity when a
neuron captures two or more observations from the training data.  Therefore,
it is still possible to have less than forty measurements.

\begin{table}[hbt]
\centering
\caption{Mean internal heterogeneity for each simulation, by topology.}
\label{ivtable3}
\begin{tabular}{|c||c|c|c|c|}
\hline
\textbf{Simulation} & Geodesic & Spherical & Hexagonal & Rectangular \\
\hline
\hline
\textbf{1} & 0.0277 & 0.0277 & 0.0285 & 0.0289 \\
\textbf{2} & 0.0281 & 0.0281 & 0.0291 & 0.0295 \\
\textbf{3} & 0.0278 & 0.0280 & 0.0286 & 0.0292 \\
\textbf{4} & 0.0280 & 0.0282 & 0.0286 & 0.0293 \\
\textbf{5} & 0.0279 & 0.0280 & 0.0289 & 0.0296 \\
\textbf{6} & 0.0278 & 0.0274 & 0.0285 & 0.0290 \\
\textbf{7} & 0.0286 & 0.0283 & 0.0294 & 0.0297 \\
\textbf{8} & 0.0284 & 0.0285 & 0.0294 & 0.0298 \\
\textbf{9} & 0.0283 & 0.0282 & 0.0293 & 0.0295 \\
\textbf{10}& 0.0285 & 0.0285 & 0.0293 & 0.0298 \\
\hline
\hline
\textbf{Combined} & 0.0281 & 0.0281 & 0.0290 & 0.0294\\
\hline
\end{tabular} \end{table}

We used several machines of varying configurations to train the forty
different SOMS.  On the fastest of those machines training one SOM took
around forty-five (45) minutes.  In contrast SOM\_PAK takes only ninety (90)
seconds to train the hexagonal SOM with the same training parameters.  This
significant difference is largely do to differences in the underling data
structures.  We traded speed for the ability to represent a SOM with any
topology.  Differences between the Python and C programing languages may also
account for significant difference in runtime.


\section{Internal heterogeneity vs. first-order neighborhood size}
\label{rdq1}
%The \emph{degree} is one measure of a neuron's centrality in a network. A
%neuron with a higher degree has higher network connectivity, and therefore is
%updated more frequently during the training process.
We hypothesized that outlying observations would migrate to less central neurons on the map, where
they encounter less competition.  We expected the internal heterogeneity
as measured by equation \ref{eqno1} to increase at these locations,
demonstrating that topological irregularity affects the placement of outliers
in the SOM.  

\begin{table}[htb]
\centering
\scriptsize
\begin{minipage}{\textwidth}
\caption{Size, mean and variance of each sample}
\label{meanvar1}
\begin{tabular}{|c||cc|cc|cc|cc|}
\hline
\textbf{Degree} & \multicolumn{2}{c|}{\textbf{Geodesic}} &
\multicolumn{2}{c|}{\textbf{Spherical}} &
\multicolumn{2}{c|}{\textbf{Hexagonal}} &
\multicolumn{2}{c|}{\textbf{Rectangular}} \\
\hline
& N & Mean (Var) & N & Mean (Var) & N & Mean (Var) & N & Mean (Var) \\
\hline
2&&&&& 20& 0.0409 (5.66E-06)& 40& 0.0378 (7.59E-06)\\ 
3&&&&& 218& 0.0371 (2.18E-05)& 880& 0.0348 (3.59E-05)\\ 
4&&&&& 489& 0.0347 (4.05E-05)& 4926& 0.0284 (6.28E-05)\\ 
5& 113& 0.0283 (5.28E-05)& 526& 0.0279 (6.43E-05)& 206& 0.0319 (3.50E-05)&&\\ 
6& 5598& 0.0281 (6.05E-05)& 4758& 0.0282 (6.05E-05)& 4954& 0.0278
(6.09E-05)&&\\ 
7&&& 417& 0.0273 (6.65E-05)&&&&\\ 
\hline
\end{tabular} \end{minipage} \end{table}

For each topology, we calculated the internal heterogeneity and degree of
all the neurons.  We then grouped the internal heterogeneity measures by degree.  These groups are
treated as representative samples from a larger distribution. Table \ref{meanvar1}
shows the details of each sample, including its size, mean and variance. 
The size is the number of observations for which we were able to calculate an
internal heterogeneity. The mean and variance capture the centrality and
dispersion of the samples.  We create a box-and-whisker diagram (box plot) for
each sample, as shown in figure \ref{boxplot}, to visualize the samples
summarized in table \ref{meanvar1}.

\begin{figure}[htb]
\centering
\subfigure[Rectangular Topology]{
  \label{boxplot:rook}
  \includegraphics[width=.45\linewidth]{rook_iv_box.png}
}
\subfigure[Hexagonal Topology]{
  \label{boxplot:hex}
  \includegraphics[width=.45\linewidth]{hex_iv_box.png}
}
\subfigure[Spherical Topology]{
  \label{boxplot:graph}
  \includegraphics[width=.45\linewidth]{graph_iv_box.png}
}
\subfigure[Geodesic Sphere Topology]{
  \label{boxplot:geodesic}
  \includegraphics[width=.45\linewidth]{geodesic_iv_box.png}
}
\caption{Box-and-whisker diagrams representing samples derived from forty
trained SOMs.  The samples within each topology were created by grouping the
internal heterogeneity of each neuron of a given degree size. The diagrams how the
centrality and dispersion of each sample.}
\label{boxplot}
\end{figure}

We see that the means of the samples seem to respond as expected for the
rectangular and hexagonal, or ``flat,''  topologies, but not in the spherical
topologies.  While this is not what we anticipated, it may suggest that the
spherical and geodesic topologies are effectively overcoming the edge problem.
The variance of the samples, however, did not respond as expected.  In the
flat topologies, the variance increased as the degree increased.  This is
possibly due to the large difference in sample size.  To verify these
conclusions we formally test for differences in means and variance between the
samples.

\begin{table}[htb]
\begin{minipage}{\textwidth}
\caption{Results of Difference of Mean Testing Within Each Topology}
\label{rlt}


  \subtable[Rectangular Topology]{
    \label{rlt:rook}
    \begin{tabular}{|c||c|c|c|}
    \hline
    Degree&2&3&4\\\hline
    \hline
    2 & (0.000008) & \textbf{0.002700} & \textbf{0.000100}\\\hline
    3 & 0.002969 & (0.000036) & \textbf{0.000100}\\\hline
    4 & 0.009364 & 0.006396 & (0.000063)\\\hline
    \end{tabular}
  }

  \subtable[Hexagonal Topology]{
    \label{rlt:hex}
    \begin{tabular}{|c||c|c|c|c|c|}
    \hline
    Degree&2&3&4&5&6\\ \hline
    \hline
    2 & (0.000006) & \textbf{0.000800} & \textbf{0.000200} & \textbf{0.000100} & \textbf{0.000100}\\\hline
    3 & 0.003810 & (0.000022) & \textbf{0.000100} & \textbf{0.000100} & \textbf{0.000100}\\\hline
    4 & 0.006253 & 0.002443 & (0.000040) & \textbf{0.000100} & \textbf{0.000100}\\\hline
    5 & 0.009008 & 0.005197 & 0.002755 & (0.000035) & \textbf{0.000100}\\\hline
    6 & 0.013067 & 0.009257 & 0.006814 & 0.004059 & (0.000061)\\\hline
    \end{tabular}
  }

  \subtable[Spherical Topology]{
    \label{rlt:graph}
    \begin{tabular}{|c||c|c|c|}
    \hline	
    Degree&5&6&7\\ \hline
    \hline
    5 & (0.000064) & 0.485500 & 0.197400\\\hline
    6 & 0.000250 & (0.000060) & \textbf{0.020000}\\\hline
    7 & 0.000674 & 0.000924 & (0.000067)\\\hline
    \end{tabular}
  }

  \subtable[Geodesic Topology]{
    \label{rlt:geodesic}
    \begin{tabular}{|c||c|c|}
    \hline
    Degree&5&6\\ \hline
    \hline
    5 & (0.000053) & 0.783900\\\hline
    6 & 0.000203 & (0.000060)\\\hline
    \end{tabular}
  }
\end{minipage}\end{table}

The results of the means test are presented in table \ref{rlt}. This table
shows the difference in means below the diagonal, the p-value above the
diagonal (with significant values in bold), and the variance of each
sample along the diagonal.  In the rectangular and hexagonal topologies, we
observe that all sample means are significantly different.  We also observed
significant differences in the variance of the samples, except the case of
degree size four and five in the hexagonal topology, where they were not
significantly different. No variances in the spherical and geodesic topologies
were significantly different.  In those topologies, the only significant
difference in means was between the sample with degree size six and seven in
the spherical topology.

%I suspect a better measure would be to use the minimum number of steps for a
%given nueron to reach every other neuron on the network.  This would vary
%significantly on the rook and hexagon topolgoies, but not very much in the
%spherical topologies.

%\begin{landscape}
%\end{landscape}

%\subsection{Restate the Questions}
%\textbf{Objective}, Compare the internal variance of observations captured by a given
%neuron to that neuron's first-order neighborhood size.
%\textbf{Question}, Does the internal variance of a neuron decrease as its first-order
%neighborhood size, or degree, increases?


\section{Internal heterogeneity vs. topological regularity}
\label{rdq2}
Above we saw that changes in a neuron's degree was related to changes in
internal heterogeneity in topologies with edges, but not in the two sphere-based
topologies. The next step is to see if internal heterogeneity changes between
topologies.  To do this we will first order our topologies by a summary
measure of the topological regularity.  For this diagnostic we use the average
closeness centrality for each topology as the summary measure.  The summary
measure and the sorting of our topologies is shown in table \ref{vardeg}.

\begin{table}
\centering
\begin{minipage}{\textwidth}
\caption{Measure of Topological Regularity and Sample Mean and Variance}
\label{vardeg}
\begin{tabular}{|c||c|c|c|}
\hline
Topology & Closeness Centrality & Mean & Variance\\
\hline
Rectangular & 0.0603 & 0.0294 &0.0001\\
Hexagonal & 0.0739 & 0.0289 &0.0001\\
Geodesic & 0.0890 & 0.0281 &0.0001\\
Spherical & 0.0906 & 0.0281 &0.0001\\
\hline
\end{tabular}
\end{minipage}
\end{table}

In this diagnostic we once again group the neurons of our trained SOMs. This
time we group them based on their topology, collapsing the neurons of the
forty SOMs into four groups.  This results in one sample for each of the four
topologies.  We test for a
difference in mean and variance between each sample using the same method of
random labeling that was applied in the previous diagnostic.  No significant
differences were found in the variances; the difference in means are presented
in table \ref{rlt:all}.  It was expected that the mean and variance of the
samples would decrease for the more regular topologies.  These results
generally support this hypothesis. We see that the rectangular topology has
the highest mean internal heterogeneity and is the least regular as measured by
closeness centrality. The geodesic and spherical topologies are the most
regular and have the lowest internal heterogeneity. These two groups display very
similar measures of closeness centrality and show no significant difference in
mean internal heterogeneity.  This suggests that even though the spherical topology
is more irregular than the geodesic topology, similar levels of quality may be
achieved.


\begin{table}
  \begin{minipage}{\textwidth}
  \caption{Results of Difference of Mean Testing Across Topologies}
  \label{rlt:all}
  \begin{tabular}{|c||c|c|c|c|}
  \hline
  \textbf{Topology}&Rectangular	&Hexagonal &Geodesic &Spherical\\\hline
  \hline
   Rectangular & (0.000064) & \textbf{0.001000} & \textbf{0.000100} & \textbf{0.000100}\\\hline
   Hexagonal & -0.000479 & (0.000064) & \textbf{0.000100} & \textbf{0.000100}\\\hline
   Geodesic & -0.001329 & -0.000850 & (0.000060) & 0.505600\\\hline
   Spherical & -0.001328 & -0.000849 & 0.000001 & (0.000061)\\\hline

  \end{tabular}
  \end{minipage}
\end{table}



\section{Visualization of internal heterogeneity}
\label{rdq3}
In order to address our research questions we first created ten synthetic
datasets.  Each dataset was created by sampling from the same data generating process
that was provided by \cite{wu2006}.  We used these datasets to train ten
SOMs for each of our topologies.  The purpose of training ten SOMs was to
produce large enough samples sizes for the difference of means and variance
testing that was necessary to formally evaluate the results of our first two
research questions.  In this section we verify that combining those
simulations was appropriate by visualizing the similarities between them.
In the remainder of this section we focus our efforts on comparing internal
heterogeneity across
topologies.  We see that little variation exists between the ten
simulations, as such we choose the first of those simulations for each topology and
explore them in more depth.
%What information can be gained from visualizing the SOM and its internal heterogeneity. 
%In order to create these visualizations we used off-the-shelf GIS packages,
%namely ESRI's ArcGIS.  Two primary challenges had to be overcome. First, to
%visualize the rectangular and hexagonal topologies we created polygons
%centered over each neuron, however, the spherical and geodesic topologies were
%slightly more complicated.  Creating the polygons for these topologies
%required that we first compute the Voronoi diagram on the surface of the
%sphere.  This is done using STRIPACK, a software program created by
%\cite{Ranka97}.  Second, ArcGIS and other common GIS packages assume Cartesian
%distances and thus can not handle polygons that cross the $180^{th}$ meridian.
%To accommodate this we split each polygon at the $180^{th}$ meridian and
%redrew it as two parts.

\begin{figure}[ht]
\centering
\begin{minipage}{\textwidth}
\subfigure[Rectangular Topology]{
  \label{ten:rook}
  \includegraphics[width=0.5\linewidth]{rook_thesis_7c.png}
}
\subfigure[Hexagonal Topology]{
  \label{ten:hex}
  \includegraphics[width=0.5\linewidth]{hex_thesis_7c.png}
}
\subfigure[Spherical Topology]{
  \label{ten:graph}
  \includegraphics[width=0.5\linewidth]{sphere_thesis_7c.png}
}
\subfigure[Geodesic Sphere Topology]{
  \label{ten:geodesic}
  \includegraphics[width=0.5\linewidth]{geodesic_thesis_7c.png}
}
\caption{Internal heterogeneity mapping for each of the forty SOMs. Darker colors
represent neurons that display larger internal heterogeneity. Neurons for which
an internal heterogeneity could not be calculated are not displayed.}
\label{ten}
\end{minipage}
\end{figure}

As expected, figure \ref{ten} reveals little variation 
between simulations of a given topology.  This homogeneity demonstrates that
our ten synthetic datasets adequately increase our sample size without introducing bias.  
Because of the apparent homogeneity between the simulations \emph{within} a
given topology, we move on to compare the first simulation \emph{across} topologies.  
Our synthetic training data consisted of seven clusters located in three
dimensional space. We examine how the SOM treats the original data through a
series of visualizations.  The component planes in figure \ref{cplanes} show
how the SOM represents each dimension of the training data.  The variations we
see in the placement of high and low values, between topologies, is caused by
the random initialization of the SOM.  However, the relationships between the
dimensions remain constant across topologies. 

\begin{figure}[ht]
\begin{minipage}{\textwidth}
  \includegraphics[width=\linewidth]{cplanes.png}
  \caption{The first (A), second (B) and third (C) component planes are shown
for the first simulation of each topology.  These component planes show how
the original dimensions are represented in the trained SOMs.}
  \label{cplanes}
\end{minipage}\end{figure}



Figure \ref{cluster} shows how the SOM detected the clustering of the input
data.  We compute the cluster each neuron represents by looking at the
observations mapped to it.  Knowing which cluster each observation belongs to
allows us to see where the clusters are being mapped to on the trained SOM. For a
given neuron we classify it by the cluster it captured most frequently. 

\begin{figure}[htb]
\centering
\begin{minipage}{\textwidth}
\subfigure[Rectangular Topology]{
  \label{cluster:rook}
  \includegraphics[width=0.5\linewidth]{rook_clusters.png}
}
\subfigure[Hexagonal Topology]{
  \label{cluster:hex}
  \includegraphics[width=0.5\linewidth]{hex_clusters.png}
}
\subfigure[Spherical Topology]{
  \label{cluster:graph}
  \includegraphics[width=0.5\linewidth]{sphere_clusters.png}
}
\subfigure[Geodesic Sphere Topology]{
  \label{cluster:geodesic}
  \includegraphics[width=0.5\linewidth]{geodesic_clusters.png}
}
\caption{Detailed internal heterogeneity mapping for each topology. Darker colors
represent neurons that display larger internal heterogeneity. Neurons for which
an internal heterogeneity could not be calculated are not displayed. The numbers
represent primary cluster mapped at that neuron.}
\label{cluster}
\end{minipage}
\end{figure}

It is interesting to note that the edges of the cluster exhibit higher internal heterogeneity.  
This is somewhat intuitive, as our clusters are normally distributed; the
majority of our observations will fall well within the regions observed in
figure \ref{cluster}. Each cluster's outliers will be pushed toward the
\emph{edges} of these regions. An observation that is on the edge of a cluster
in the original input-space is further away from the other observations in the
input-space.  Therefore, one would expect that observation to also be near the
edge of a cluster in the SOM space.  In order to represent a three dimensional
cluster in two dimensions, the SOM must compress the edges of the clusters
more than their centers.  This explains the higher internal heterogeneity near
the edges of the clusters. 

%In the hexagonal and rectangular topologies,
%some of these cluster regions share an edge with the topology itself.  This
%allows their outlying observation to interact less with the interior neurons.
%In the spherical and geodesic topologies outliers that are pushed toward the edge of
%their cluster's region on the map are forced to interact with other neurons.
%Our particular dataset consisted of seven cluster in the three dimensions.
%Six of those clusters were centers at the extremes of each axis, as such a
%topology which allows these extremes to be represent in two dimensions may be
%more appropriate. 

%The particulars of synthetic dataset in combination with these visualizations
%may raise questions about the suitability of sphere based SOMs in certain
%applications.  Specifically, extreme observations in our dataset (those
%furthest from the origin), may be well represented on the edge of the flat
%topologies where there are furthest away from their counter parts in the other
%extreme.  This may hold true in the spherical topologies as well if these
%extreme observations are antipodal, in fact it might be even better
%represented.


\section{The Utah-Hawaii Problem}
\label{states}
This research topic was initially inspired when an unwitting professor made
the assumption that Utah and Hawaii were similar, because they both fell on
the same neuron in a trained SOM.  This raised the question of whether Utah and
Hawaii were in fact similar, or if this relationship in the SOM was merely
caused by an edge effect.  It turns out the answer is slightly more
complicated.  Utah and Hawaii are in fact each other's nearest neighbor, as
defined by the census variables used by \cite{skupin08}.  However, as shown in
Figure \ref{som:states} and explained in section \ref{bg:edge}, both of these
observations can be considered outliers.  Neither fits the trained SOM well and both are
far from the mean of all the states.

\begin{figure}[h]
\centering
\includegraphics[width=.9\linewidth]{statesSphere.png}
\caption{An example of how spherical SOM addresses the boundary effect found in
traditional SOM.}
\label{sphere:states}
\end{figure}

In Figure \ref{sphere:states} we present the same data trained on a 400 neuron
spherical SOM.  The topology for this SOM is based on \cite{Rakhmanov94}.  We
use the same coloring scheme and labeling used in Figure \ref{som:states}.
Darker neurons are further from the mean of the states, while lighter neurons
are more similar to the mean.  The size of the symbol represents the distance
between each state and the mean of the states.  The bold labels highlight the
five observations that are furthest from the mean, while the underlined labels
highlight the five states closest to the mean.  We find, not surprisingly, that
Utah and Hawaii are separated in this SOM.  This implies that edge effects indeed
played a large role in the placement of these two observations in the hexagonal
SOM.  As suggested in section \ref{bg:sphere}, we also find these and other
outlying observations occupy more space in the trained spherical SOM than in
the hexagonal SOM.  States closer to the mean such as Ohio, Missouri and
Kentucky are distinctly more clustered in the spherical SOM than in
the hexagonal SOM.  It is interesting to note that the orientation between
these three states remained largely unchanged between the two maps.

