\documentclass[11pt]{article}
\setlength{\topmargin}{-.2in}
\setlength{\oddsidemargin}{-0cm}
\setlength{\evensidemargin}{-1cm}
\setlength{\textwidth}{16.3cm}
\setlength{\textheight}{22.3cm}
\usepackage{graphicx}
\usepackage{subfigure}
\graphicspath{{figures/}}
\usepackage[round]{natbib}
\title{Effects of Irregular Topology in Non-Planar SOM Variants}
\author{\sc{Charles R. Schmidt}\\Regional Analysis Laboratory\\Department of Geography\\San Diego State University}
\date{\today} %\date{January 29th, 2007}
\begin{document}
\maketitle
\begin{abstract}
The development of the spherical SOM has been driven by the border effects
observed in traditional SOM.  Two problems exist with the Spherical SOM. The
first is the level of control over the network size. The second is the
topologically induced errors caused by the arrangement of neurons on the sphere.
Both of these problems stem from the problem of uniformly distributing points on
a sphere. These problems will be investigated through the introduction of a new method for
testing topologically induced errors. The method first analyzes  the neural
network to find topological mis-matches, next we train the network with an
overwhelming about of synthetic data.  Through a series of simple plots we can
then compare each neurons internal variance as defined by the variance of the
observations that best fit that neuron with suchs metrics as neighborhood
influence, number of child observations, etc.
\end{abstract}


%\section{Problem Statement}
%1. Introduction (10\%)
%\\2. Background and Lit Review (30\%)
%\\	\ldots Problem Statement
%\\	\ldots Lit Review
%\\3. Research Design / Plan / Metodology (50\%)
%\\4. Significance and Limitations (10\%)
%\\5. Timetable

\section{Introduction}
Using a spherical lattice is widely suggested as a solution to the boundary
effect found in the traditional Self-Organizing Maps \citep{ritter99, boudjemai2003,
sangole03, Wu:2006lr, Nishio:2006fk}.  However, the use of the spherical lattice
introduces a new problem.  Save the five platonic solids, distributing points on
a sphere will always result in irregular topology \citep{ritter99}.  The classic
method for generating a spherical lattice is to tessellate the sides a platonic
solid.  When tessellating the icosahedron, as described by \cite{Wu:2006lr}, the
resulting topology will always consist of \(12\) pentagons and \(N-12\) hexagons.
Where \(N\) is the total number of sides, or neurons.  The main drawback of this
method is the finite control over \(N\), which grows at a rate of \(f^2*10+2\),
where \(f\) is the frequency of the tessellation \citep{Wu:2006lr}.  There has been
little discussion on the effects of irregular topology in Spherical SOM. The goal of
this research is to investigate these effects in order to determine the usefulness of
slightly more irregular topologies which offer greater control over \(N\).
%Find a way to incorporate finite control over N into this sentence.  A question
%that has not yet been answered is to what degree does the irregular topology
%effect the Spherical SOM.

\section{Background and Lit Review}
The Self-Organizing Map (SOM) is an unsupervised competitive learning process
developed by Teuvo Kohonen as a technique to analyze high dimensional data sets.
The SOM algorithum uses an artificial neural network to organize high
dimensional data onto a low dimensional lattice, or map, of neurons.  Each
neuron contains a reference vector that can be considered to model a portion of
the input space. Before training these neurons are initialized, most commonly to
random values.  During the training process a randomly selected input vector
searches the map for its best matching unit (BMU), that is the neuron to which
it is most similar. The BMU and its neighborhood, as defined by a neighborhood
function, are than adjusted to better match that observation
\citep{Kohonen2000}.  The training process is repeated a predefined number of
times, or ideally until the map converges.  The traditional SOM is laid out on a
two dimensional plane using either a rectangular or hexagonal topology.
According to \cite{Wu:2006lr} the hexagonal structure is more uniform and
generally preferred.

\subsection{Boundary Effect}
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{gridedge_grey.pdf}
\caption{Fifty states plus D.C. mapped onto SOM trained with the first thirty-two census
variables.  Darker neurons have a relatively large differences from the mean of
the states, while lighter neurons are relatively closer.  Larger dots represent inputs
that were poorly fit to the map, small smaller dots show inputs that were better bit to the map.}
\label{figure1}
\end{figure}

One obvious drawback of building the neural lattice in a discrete euclidean 
plane is the boundary of the resulting lattice.  A neuron located on the boundry has 
fewer neighbors and thus fewer chances of being updated \citep{Wu:2006lr}.  
As observed in Figure \ref{figure1}, neurons in the center of the 
map tend to better represent the mean of the input-space.  This is arguably caused 
by outliers being pushed to the edges of the map, where they encounter fewer 
competing signals.

The toroidal som was introduced by \cite{li1993} and removes this drawback.
However, the torus is not hugely effective for visualization, as maps generated from 
a torus are not very intuitive \citep{ito2000,Wu:2006lr}.  \cite{ritter99} describes 
the torus as being topologically flat and suggests that a curved topology, such 
as that of a sphere, may better reflect directional data.  A sphere also results in a 
more intuitive map, since we are accustomed to looking at maps based on a sphere.

\subsection{Spherical SOM}
\cite{ritter99} first introduced the spherical SOM and several
enhancements have since been suggested 
\citep{boudjemai2003,sangole03,Nishio:2006fk,Wu:2006lr}.  A good
comparison of these enhancements can be found in \citep{Wu:2006lr}.  All of
these methods derive their spherical structure through the tessellation of a
polyhedron as originally proposed by \citeauthor{ritter99}.  \cite{Wu:2006lr}
point out the importance of a uniform distribution on the sphere and that it is
preferable for all neurons to have an equal number of neighbors and to be
equally spaced.  They find generally that the tessellation method best
satisfies these conditions and specifically that the icosahedron is the best
starting point \citep{wu2005}.

\subsubsection{Network Size}
%Add background material on why Network Size is important.  The need for big and small networks.
%Waiting for Kohonen Book!!!	
The literature offers little theoretical guidance on network size
\citep{cho1996}.  \cite{toolbox} suggests simply using a network
size of \(5*\sqrt {n}\), where \(n\) is the number of observations.
The tessellation method used by the class of spherical SOMs
based on Ritter's work results in a network size that is a function of the
tessellation frequency and therefore grows exponentially. In practice 2D
Euclidean SOMs also offer limited control over network size, as it is
undesirable to have one dimension dramatically larger then the other.
\cite{Nishio:2006fk} try to address the issue of network size granularity by
departing from the tessellation method and suggesting the use of a partitioned
helix to uniformly distribute any number of neurons on a sphere.  A similar
method was dismissed by \cite{wu2005} for failing to satisfy the uniformity
conditions.  \citeauthor{Nishio:2006fk} seem to have addressed variance in 
nearest neighbor distances, but the issue of having a uniform number of direct
neighbors is still not addressed.

\subsection{Conclusion}
%\subsubsection{Neuron Spacing and Neighborhood Size Variance}

Tessellation of the icosahedron results in a network of neurons, each of which
have exactly six neighbors, save the original twelve which each have five.
This is very close to the ideal structure in which every neuron would have
exactly six neighbors.  This structure has very low variances in both neuron 
spacing and neighbor size.  The literature has attached too much importance 
to the variance of neuron spacing, specifically as a metric to compare different 
network topologies.  

Based solely on measures of neuron spacing \cite{wu2005} dismissed a method
proposed by \cite{Rakhmanov94} for distributing points on a sphere.  Similarly 
\cite{Nishio:2006fk} use these variance measures to support their helix algorithm 
for distributing points on a sphere.  Table \ref{table1} shows that these metrics can 
be misleading and may not be comparable across topologies.  The traditional 
rectangular and hexagonal topologies have no variance in neuron spacing and 
the generally preferred hexagonal structure displays greater variance in neighborhood
size than the rectangular structure.  The torus by comparison would have variance in 
neuron spacing, yet no variance in neighborhood size.  The distance between two 
neurons is only considered during the formation of the neuronal network.  At this stage
the spacing is significant as it plays a part in determining neuron adjacency. However 
using this measure to evaluate potential topologies for use in SOM may be misleading.

\begin{table}[htbp]
\caption{Variances in Topologies}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Topology&Grid Size&Neuron Spacing&Variance in Neighborhood Size\\
\hline
Rectangular&9x18&1&0.2716\\
Hexagonal&9x18&1&1.2138\\
Tessellation&162&0.25319 - 0.31287& 0.0686\\
Rakhmanov&162&0.15779 - 0.30069& 0.2908\\
\hline

\end{tabular}
\end{center}
\label{table1}
\end{table}%

Methods, for distributing points on the sphere, which allow for fine grained
control over network size produce slightly more irregular topologies.  However,
no discussion of these irregularities or their effects on SOM training has
occurred in the literature. Given that limited theoretical guidance is available
for choosing network size the desire for finer control over the network size,
should not be overlooked. In particular for a larger SOM the ideal network size
may not be achievable via tessellation of the icosahedron.

%\section{Proposed Topology}
%The Rakhmanov algorithm has the ability to distribute any number of points onto
%the surface of a sphere, which allows for the finest possible level of control
%of network size, even greater than that of the traditional euclidean based SOM
%\citep{Rakhmanov94}.  There are two problems with the \citeauthor{Rakhmanov94}
%algorithm. The first is the variance in the number of direct neighbors for any
%given neuron.  The second is variance in the distance between those neurons.


%Irregularities in the spatial distribution of neurons exhibited by the Rakhmanov
%algorithm allows a given neuron's neighboring points to be easily ranked
%and sorted by their distance to the neuron in question.  The ranked and sorted
%neighbors can be further separated into distinct classes, or orders, such that
%the first order contains exactly \begin{math}1\times6\end{math} neurons and
%the \textit{n\textsuperscript{th}} order contains exactly
%\begin{math}n\times6\end{math} neurons.  Furthermore, neurons will be assigned
%a distance relative to the central neuron and equal to that of their order.
%For example, the six closest neurons will all be assigned a distance weight of
%unity, while the next twelve closest neurons will be assigned a distance
%weight of two and so on and so forth.  This follows from the properties of the
%standard hexagonal topology used traditionally in SOM and provides the
%foundation for a spherical SOM which mimic's that behavior.  This approach
%shall be referred to as HS-SOM.

\section{Data and Methodology}
The basic problem of the "boundary effect" is that neurons on the edge have
fewer neighbors. Yet there are only five possible arrangements of points on a
sphere such that all points have the same number of neighbors.  Any spherical
lattice consisting of more then twenty (dodecahedron) neurons will contain
topological irregularities.  This is to say that not all neurons will have the
same number of neighbors.  The importance of these irregularities and the 
magnitude of their effects on SOM training is not known.  This goal of this research
is to determine whether more flexible network structures may be used in spherical 
in SOM without introducing significant errors. To accomplish this goal, basic methods 
in network analysis with be combined with the result from several empirical training
runs each utilized different topology.  

\subsection{Synthetic Data}
In order to have a basis for comparison 



In order to find the locations of these mis-matches a
first order adjacency matrix \(A\) will be constructed for each network
topology.  The sum of \(A_{*,n}\) represents the number of neurons that consider
neuron \(n\) to be a neighbor.  In a perfectly edgeless topology the variance of
these column sums would be \(0\).

This variance can be used as a global distortion measure comparable across any SOM, both spherical and 2D of a given network size.

It is known that for a tessellated icosahedron local distortions
will be observed at exactly twelve locations on the surface, around the twelve
original vertices.  For a hexagon lattice in 2D euclidean space the
distortions will be observed at the edges of the lattice.  

It is hypothesized that for HS-SOM, distortions will be less uniform in their spatial
distribution and their overall magnitude will be less then those exhibited by
either GeoSOM or SOM\_PAK.

Further the results can be visualized to show the exact location of
topologically caused errors on the lattice.  Since we know the distortions
will occur in GeoSOM, we can use these results to help form understanding and
possibly lead to an intuitive mathematical weighting scheme for GeoSOM such as
that proposed by \cite{Kohonen2000} for 2D SOM. The method allows us to
examine the suitability of the various algorithms for uniformly distributing
points on a sphere for use in HS-SOM.

For a benchmark, the experiment is carried out over two 3x3 grids, one of
hexagonal topology, the other of rectangular topology. The neighborhood
constant is set to unity; meaning only immediate (rook contiguity) neighbors
are included. The input vector's sole value shall equal two. After each
neuron was used once as a pseudo BMU (nine training cycles) the variance of
the rectangular grid was 0.4444, while the variance of the hexagonal grid was
1.5802. As shown in figures \ref{figure2} and \ref{figure3} the hexagonal lattice
experienced higher overall values.

It should be noted that neither the potential harm nor the possible benefits
of distortions have note been explored here.  It is feasible that distortions
may help the convergence process and eliminating them entirely could
unnecessarily prolong training.

\begin{figure}
\centering
\includegraphics[width=0.4\linewidth]{figure_hex.png}
\caption{Lower numbers and lighter colors indicate topological distortion.
In the case of this specific example eight is the ideal value for all neurons
in the hexagonal lattice, two for yourself plus one for each neighbor.  For
the rectangular lattice the ideal value is six.}
\label{figure2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.4\linewidth]{figure_rect.png}
\caption{Lower numbers and lighter colors indicate topological distortion.  In
the case of this specific example eight is the ideal value for all neurons in
the hexagonal lattice, two for yourself plus one for each neighbor.  For the
rectangular lattice the ideal value is six.}
\label{figure3}
\end{figure}

\section{Limitations}
The primary limitation of the HS-SOM method is speed in neighborhood
searching. Time complexity for neighborhood searching in HS-SOM is
\begin{math}O(N)\end{math}, while GeoSOM runs in
\begin{math}O(n)\end{math}\footnote{\textit{N} is the network size and
\textit{n} in the number of neurons in the current
neighborhood} \citep{Wu:2006lr}. Time and space complexity can be reduced by
caching neighborhood calcuations on-the-fly out to the current neighborhhod
size, as opposed to precalcuting neighborhoods out to initial neighborhood
size.
\section{Expected Results and Future Work}
\begin{itemize}
\item A new tool for measuring topological preservation will be introduced.
\item HS-SOM will prove to have less distortion then 2D euclidean SOM and hopefully GeoSOM.
\item Implement the validation method and test the GeoSOM and HS-SOM using the Rakhmanov and helix methods.
\item Develop more diagnostics, such as the reverse quantization error visualizations.
\end{itemize}



%\section{Bibliography}
\bibliographystyle{apalike}
\bibliography{som}
\end{document}
