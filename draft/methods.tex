\chapter{METHODOLOGY}
This chapter is composed of two sections.  The first describes the diagnostics
developed for evaluating network topologies.  The second describes an
empirical study that will implement these methods in order to evaluate the
utility of topologies that allow for greater control over network size.

\section{Diagnostics}
Three diagnostics are developed to explore the effect of irregular topology on
spherical SOM.  The first diagnostic will be used to address the research
question regarding the internal variance and neighborhood size.  The second
diagnostic will address the question concerning internal variance and
topological irregularity.  The third tool will help visualize the patterns
between internal variance and topology.

\subsection{Internal variance vs. first-order neighborhood size}
\label{q1}
This diagnostic will compare the internal variance of each neuron against its
first-order neighborhood size.  In traditional SOMs, outlying observations are
pushed to the edge of the map where they encounter fewer competing signals. A
prime example of this is the ``Utah-Hawaii'' case shown in Figure
\ref{figure1}.  Relying only on the SOM, one would be left to believe that the
two states are similar. Upon closer inspection we see that the QError from
Utah to the neuron is $1.509$, the QError from Hawaii to the neuron in
$1.505$, but the QError from Utah to Hawaii is $3.014$. In this case only Utah
and Hawaii were mapped to that neuron.  In a case where multiple observations
land on the same neuron, it is possible to measure average pairwise QErrors
between those observations.  This gives us a notion of internal variance for
each neuron. It would be expected that in traditional SOMs neurons closer to
the edge will have higher internal variances. This can be extended to
spherical SOMs by comparing the degree of a neuron ($deg(m_i)$ or the number of
adjacent neurons) to its internal variance.  The degree of each neuron can
easily be calculated by taking the column sums of the first-order adjacency
matrix ($A$).

Once the internal variance ($var(m_i)$) and degree ($deg(m_i)$) of each neuron
have been calculated, the neurons can be separated into a small number of
groups based on the degree \footnote{For most topologies the number of
different degrees will be limited to three or four.}.  The variance and mean
will be calculated for each of these groups.  The expected result is that
variances and means of the groups will decrease as the degree increases.  This
hypothesis will be tested using random labeling as described by \cite{siss2004}.
The result will also be visualized using a box plot.

\subsection{Internal variance vs. centrality}
The degree of a node on a network is a measure of its centrality, or
importance. Nodes with more connections are thought to be more central to
network and have a larger influence than nodes with fewer connections. As an
artifact of the training process observations that are more average than
others tend to be centralized.  The observations that sound them tend to be
more extreme.  If you refer back to figure \ref{figure1}, you'll notice that
observations with smaller symbols are closer to the mean of all the
observations and that these observations have been centralized in the network.

Using the degree as a measure of centrality does not capture this picture
well, as neurons near the edge can still have a large degree.  A way to
capture this effect would be to look at closeness centrality.  This is, the
inverse of the average distance of a neuron to every other neuron on the
network.


This diagnostic will compare the internal variance of each neuron against a
summary measure of centrality for its associated topology. Centrality measures
the importance of a node in the graph.  The most central node is 

As mentioned above the
degree of each neuron can be calculated by taking the column sums of $A$.  A
completely regular network topology (i.e. the torus) will have no variance
between these column sums.  For irregular networks the variance between these
column sums gives us a measure of irregularity.  An alternative to using a
measure of regularity would be a measure of connectivity or centrality.  
\cite{florax95} outline four measures of connectivity and analyze their
sensitivity to network size.
%There are many ways to classify the connectivity of a network; such summary measures.
For each topology we can compare the internal variances as described above
against a measure that summarizes the given topology's regularity.

This diagnostic is evaluated in much the same way as the last diagnostic.  
The internal variances are this time grouped by their topology.  We can then
compare the variances of internal variances and the means of the internal
variances across topologies.  It is expected that the distribution of internal
variances will be narrower for groups trained on more regular topologies.
It is further hypothesized that the means of these internal variances will
decrease when the network is more regular, or when there is less variance in
the column sums of $A$.  These assumptions will be tested using
a \emph{t-test} on the means and an \emph{F-test} on the variances.

\subsection{Visualize internal variance mapping}
Visualizing the internal variance may yield insight into how irregular topology
effects the SOM.  Once the internal variance of each neuron has been calculated
we can use the values to color or shade a map of the given topology.  The degree
of the neurons can be visualized using proportional symbols to help show
patterns between internal variance and irregularity.

\section{Empirical Analysis}
The empirical analysis consists of three main tasks.  The first is to create
synthetic data suitable for the diagnostics described above.  The second task is
to train multiple SOMs, each of with a different topology type. The third task
will be to apply the diagnostics and interpret the results.

\subsection{Synthetic Data}
%Comment from Skupin...
%This section is obviously leaving most of the specifics of the synthetic data
%generation out, which is problematic. I'm willing to go along with this for
%the proposal though, unless it gets raised by the third committee member
In order to test the methods described above, I will generate high-dimensional
synthetic data with known properties.  Knowing the properties of the training
data allow us to systematically compare the diagnostics under several different
topologies.  To ensure that we can calculate an internal variance for each
neuron, I will generate the data so as to increase the probability that each
neuron will be occupied by more than one observation.

The data will be generated using a Gaussian cluster generator as described by
\cite{handl}. The generator creates clusters by pulling from multivariate
normal distributions.  Clusters are not allowed to overlap and no random noise
is introduced into the data.  The properties such as the number of clusters
and the number of dimensions will be decided through experimentation.  After
the data is generated, it is scaled from zero to one and the order of
randomized.  We will create a number of different data sets and use them to
train various SOMs.  The mean internal variance of each SOM will be looked at
to determine how it responds to the properties of the training data.

\subsection{Training}
The diagnostics must be given a trained SOM for each topology to be compared. To
yield any meaningful results those SOMs must be trained with comparable
parameters. Most parameters can simply be set to the same value for each SOM.
However, special consideration must be given to network size.  As shown in
Figure \ref{fig:nSize}, topologies differ in terms of achievable network size.
This analysis will include the following topologies:
\begin{itemize}
\item Rectangular
% Comment by Skupin note addressed here.  Need to come up with better
% name...
\item Hexagonal
\item A topology based on \cite{Rakhmanov94}
\item The Helix topology proposed by \cite{Nishio:2006fk}\footnote{Currently
there is some uncertainty about the ability to include the Geodesic and Helix
type topologies given the complexities involved with their implementation.
However, an additional goal of this project to provide a framework on which new
topologies can be easily implemented and tested by future researchers.}
\item The Geodesic topology proposed by \cite{wu2006}\footnotemark[2]
\end{itemize}

\subsection{Diagnostics}
The first diagnostic will yield a set of results for each topology tested.  These
results will be analyzed in order to address the first research question of this
paper.  The second diagnostic provides one set of results.  These results will
be analyzed to address the second research question.  The final diagnostic will
return a visualization for each topology tested.  The usefulness of these
visualizations is a research question in itself.  The expectation is that the
visualizations will show patterns of internal variance related to irregularities
in the network topology.

