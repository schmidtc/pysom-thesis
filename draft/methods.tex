\chapter{METHODOLOGY}
This chapter is composed of four sections.  Section \ref{meth:diag} describes
three diagnostics developed for evaluating network topologies.  The
diagnostics are applied to a series of trained SOMs.  Synthetic training data,
as described in section \ref{meth:data}, is used as our training data.  The
parameters of the training, section \ref{meth:train}, are consistent across
each trained SOM. Finally, in section \ref{meth:som} we implement our own
graph based implementation of SOM in order to test different topologies.

\section{Diagnostics}
\label{meth:diag}

In traditional SOMs, outlying observations are pushed to the edge of the map
where they encounter fewer competing signals.  A prime example of this is the
``Utah-Hawaii'' case shown in Figure \ref{som:states}.  Relying only on the
SOM, one would be left to believe that the two states are similar.  Recalling
that the QError measures the distance between two vectors in attribute space,
we see that the QError from Utah to the neuron is $1.509$, the QError from
Hawaii to the neuron in $1.505$, but the QError from Utah to Hawaii is
$3.014$. In this case only Utah and Hawaii were mapped to that neuron.  In a
case where multiple observations land on the same neuron, it is possible to
measure average pairwise QErrors between those observations.  This gives us a
notion of internal heterogeneity \(IH\) for each neuron.  We define the
internal heterogeneity of neuron \(i\) as,
 \begin{equation}
   {IH_i} = \frac{2}{{n_i}^2-{n_i}}\sum_{j=1}^{n_i}\sum_{k=j+1}^{n_i} ||{x_{ij}}-{x_{ik}}||
 \label{eqno1}
 \end{equation}
where, \(n_i\) is the number of observations mapped to \(i\), and \(x_i\) are
the input vectors mapped to \(i\).  For any neuron that captures more then one
observation, this measure tells how dissimilar those observations are.

The edge effects in SOM make it clear that the compression of the input-space is
not uniform through out the map.  While outliers being pushed to the edge of
the SOM is not necessarily an undesirable outcome, it is important understand
when and where information is being compressed.  This variable compression of
the input-space is what allows the SOM to represent high-dimensional data, but
it can also mislead the viewer.  Observing the internal heterogeneity of
neurons map shed light on the patterns of compression.

More specifically we have developed diagnostics that try to explain how the
topology of the SOM effects this compression.  We compare the internal
heterogeneity at the scale of the neuron and the overall map.  Each diagnostic
was designed to answer a specific research question.  The first diagnostic
addresses the research question regarding the internal heterogeneity and
neighborhood size.  The second diagnostic addresses the question concerning
internal heterogeneity and topological irregularity.  The third helps
visualize the patterns between internal heterogeneity; the usefulness of which
is examined in the next chapter.

\subsection{Internal heterogeneity vs. first-order neighborhood size}
\label{q1}
This diagnostic compares the internal heterogeneity of each neuron against
then neuron's first-order neighborhood size.  It would be expected that in
traditional SOMs neurons closer to the edge, those with fewer neighbors, will
have higher internal heterogeneity. Neurons on the edge of the traditional
(rectangular and hexagonal) topologies are considered more irregular, because
they have few neighbors then neurons inside the edge.  This can be extended to
spherical SOMs by considering the degree of any given neuron.  The degree of a
neuron $deg(m_i)$ measures the number of adjacent neurons.  If the
relationship between $IH_i$ and $deg(m_i)$ is consistent across topologies,
neurons with lower degrees should display higher internal heterogeneity.

To implement this diagnostic we calculate the internal heterogeneity ($IH_i)$)
and degree ($deg(m_i)$) of each neuron. The neurons are then separated into a small number of
groups based on the degree.  For most topologies the number of
different degrees will be limited to three or four.  The variance and mean
is calculated for each of these groups.  The expected result is that
variances and means of the groups will decrease as the degree increases.  This
hypothesis is tested using random labeling as described by \cite{siss2004}.
In random labeling, we randomly assign our calculated $IH$ values to the
different groups and recompute the mean and variance.  We do this many times,
9999 in our case, in order to approximate the true distribution. Finally we
calculate a pseudo p-value by comparing our observed mean and variance values
with the simulated distributions.  The results are also visualized using
box-and-whisker diagrams. Box-and-whisker diagrams, or box plots, show the
properties of a distribution.  The diagram shows the mean, first and second
standard deviations, and outliers that extend beyond the second deviation.

\subsection{Internal heterogeneity vs. topological regularity}
The degree of a node on a network is a measure of its centrality, or
importance. Nodes with more connections are thought to be more central to the
network and have a larger influence than nodes with fewer connections. As an
artifact of the training process observations that are more average than
others tend to be centralized.  The observations that surround them tend to be
more extreme.  If you refer back to figure \ref{som:states}, you'll notice
that observations with smaller symbols are closer to the mean of all the
observations and that these observations have been centralized in the network.
Using the degree as a measure of centrality does not capture this picture
well, as neurons near the edge can still have a large degree.  A way to
capture this effect would be to look at closeness centrality, which is the
inverse of the average distance of a neuron to every other neuron on the
network.


This diagnostic compares the internal heterogeneity of each neuron against a
measure of regularity for its associated topology.  As mentioned above the
degree of each neuron measures the number adjacent neighbors.  A
completely regular network topology (i.e. the torus) has no variance
between these measures.  For irregular networks the variance between these
column sums gives us a measure of irregularity. There are many alternative
ways to classify the connectivity of a network; \cite{florax95} outline four
such summary measures.  For each topology we can compare the internal
heterogeneity as described above against a measure that summarizes the given
topology's regularity.

This diagnostic is evaluated in much the same way as the last diagnostic.  
The neurons are this time grouped by their topology.  We can then
compare the variances of internal heterogeneity and the means of the internal
heterogeneity across topologies.  It is expected that the distribution of internal
variances will be narrower for groups trained on more regular topologies.
It is further hypothesized that the mean of internal heterogeneity will
decrease when the network is more regular, or when there is less variance in
the column sums of $A$.  These assumptions are tested using random labeling
and visualized with box plots.
%These assumptions will be tested using a \emph{t-test} on the means and an \emph{F-test} on the variances.

\subsection{Visualize internal heterogeneity mapping}
Visualizing the internal heterogeneity may yield insight into how irregular topology
effects the SOM.  Once the internal heterogeneity of each neuron has been calculated
we can use the values to color or shade a map of the given topology.  
The degree of the neurons can be visualized using proportional symbols to help show patterns between internal heterogeneity and irregularity.

\section{Data}
\label{meth:data}
%Comment from Skupin...
%This section is obviously leaving most of the specifics of the synthetic data
%generation out, which is problematic. I'm willing to go along with this for
%the proposal though, unless it gets raised by the third committee member
The internal variance measure is sensitive to both the properties of the SOM
and the properties of the training data. Therefore, a dataset with uniform
properties is needed. We follow the method for generating uniform synthetic
data used by \cite{wu2006}.  Their method creates seven clusters in three
dimensions.  Each cluster is normally distributed and has a standard deviation
of one.  The uniform clusters generated by this method allow us to
systematically compare the diagnostics under several different topologies.  To
ensure that we can calculate an internal variance for each neuron, we create
approximately $25,000$ observations in each dataset.  As descriced in the next
section, our SOMs have either 642 or 644 neurons (depending on the topology).
Having a large number of observation relative to the number of neurons will
force the SOM to preform clustering, increasing number of neurons for which
the internal heterogeneity can be computed.


%We will create a number of different data sets and use them to train various SOMs.  

%the data is generated in such a way to increase the probability that each neuron will be occupied by more than one observation.

%Initially the synthetic data for this thesis came from a Gaussian cluster
%generator which creates clusters by randomly sampling from multivariate
%normal distributions \citep{handl}.  \citeauthor{handl}' method to keep
%clusters from overlapping is to create one cluster at a time, with each new
%cluster checked to see if it overlaps with an existing cluster. If it does,
%it is rejected.  The generator continues until the desired number of
%non-overlapping clusters has been reached.  This method tends to create
%clusters of very different shapes and sizes (or extents).

%The limit to using this method for creating data became evident when we
%realized that the internal variance measure was being affected by the
%structure of the clusters.  The internal variance essentially looks at the
%portion of a cluster that is mapped to a particular neuron and measures the
%density.  Because we specify that each cluster contain an equal number of
%observations, they tend to get equal representation (in terms of number of
%neurons) on the trained SOM.  The result of all this is that the smaller,
%more dense clusters display very low internal variance relative to the
%larger, less dense clusters.  While this may have interesting consequences in
%other applications, because of these effects on the internal variance our
%ability to determine how changes in the topology are affecting the meassure.
%because it interferes with the measurement of internal variance, we had to
%adopt another method of synthetic data generation. 



\section{SOM Training}
\label{meth:train}
Before we can go on to address the research questions we need to train a
series of SOMs.  We train SOMs using four different topologies:
\emph{rectangular, hexagonal, geodesic sphere} and \emph{spherical}.  The spherical
topology is based on a method, developed by \cite{Rakhmanov94}, for
distributing an arbitrary number of points on to the surface of a sphere.
Delaunay triangulation is then applied to these points, producing a
topological structure.  To yield meaningful results these SOMs must be trained
with comparable parameters.  The literature provides many rules of thumb for
training a SOM: each SOM is trained in two stages, the first of which uses a larger
initial learning rate and neighborhood search radius with a small number of
training steps; the second stage uses a lower initial learning rate and
neighborhood search radius, but extends the length of training.
\\
First Stage Parameters:
\begin{itemize}
  \item Initial neighborhood search radius of 50\%, which decreases during training. 
  \item Initial learning rates of 0.04 which decreases during training.
  \item 100,000 training steps.
\end{itemize}
Second Stage Parameters:
\begin{itemize}
  \item Initial neighborhood search radius of 33\%, which decreases during training. 
  \item Initial learning rates of 0.03 which decreases during training.
  \item 1,000,000 training steps.
\end{itemize}

As shown in Figure \ref{fig:nSize}, topologies differ in terms of achievable
network size.  For comparability, the network size of each SOM needs to be as
close as possible.  The achievable network size for the geodesic SOM is the
most limiting of the topologies we test. We chose the eighth frequency
geodesic sphere, which has 642 nodes, which is relatively close to the
644-node hexagonal and rectangular topologies achieved when the dimensions are
set to \(28x23\). Finally, the spherical topology was set to 642 nodes.

One problem that we face is a small sample size when the neurons of a given
SOM are grouped by their degree.  For example, the four corners of the
rectangular topology are the only neurons that have a degree of two.  The rest
of the neurons have three or four neighbors depending on whether or not they
are on the edge. To address the problem of small sample size for topologies
with relatively few neurons of a particular degree, we will increase
the sample size by combining the results of many SOMs.

\section{Graph based implementation of SOM}
\label{meth:som}
The SOM can be represented in what \cite{kohonen1996} calls a codebook.  The
codebook can be stored as a simple text file.  The first line describes topology type and
other parameters of the SOM.  Each subsequent line lists the values for the
parametric reference vectors, $m_i$, of the SOM's neurons.  The diagnostics
above operate on these codebook files, a graph based representation of the
topology, and the original training data.  \citeauthor{kohonen1996}'s
implementation of SOM, SOM\_PAK, could easily be used to provide the necessary
codebook files for the rectangular and hexagonal topologies.  However, 
implementations of the geodesic and spherical SOMs were not readily available at the
time this thesis was written.

In order to test these topologies we wrote our own implementation of SOM.  We
created this implementation of SOM to be topology agnostic.  We built our
neighborhood functions on top of an existing graph library.  This allows us to
test any topology for which we can create a graph structure.  Apart from our
neighborhood search functions, our implementation follows the original
incremental SOM algorithm that as described by \cite{Kohonen2000}.  Our code
is written in the python programing language and is provided in Appendix
\ref{append:somcode}.

We provide utility functions to create the graph structures for both the
rectangular and hexagonal topologies.  To create the graph structure for the
geodesic topology we use the ``dome'' software package, which outputs the
point coordinates in ``XYZ'' format for each neuron \citep{dome}.  These
coordinates are fed into a modified version of the ``sxyz\_voronoi'' program,
which computes both the Voronoi cells and their complement, the Delaunay
triangulation, on the sphere\citep{Ranka97}.  The Delaunay triangulation provides the graph
structure between the neurons of the geodesic SOM.  A similar process is used
for the spherical topology. In this case we wrote a python implementation of
the method for distributing points on a sphere that was introcuded by
\cite{Rakhmanov94}.  Once again the locations are fed into the modified
version of ``sxyz\_voronoi''.  The modifications to ``sxyz\_voronoi'' change
only the way it prints output, making it more usable for both creating the
graphs and for visualization.a
