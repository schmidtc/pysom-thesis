\documentclass[12pt]{article}
\setlength{\topmargin}{-.2in}
\setlength{\oddsidemargin}{-0cm}
\setlength{\evensidemargin}{-1cm}
\setlength{\textwidth}{16.3cm}
\setlength{\textheight}{22.3cm}
\usepackage{graphicx}
\usepackage{subfigure}
\graphicspath{{figures/}}
\usepackage[round]{natbib}
\title{Effects of Irregular Topology in Non-Planar SOM Variants}
\author{\sc{Charles R. Schmidt}\\Regional Analysis Laboratory\\Department of Geography\\San Diego State University}
%\author{\sc{Charles R. Schmidt}\\REGAL}
%\date{January 29th, 2007}
\begin{document}
\maketitle
\begin{abstract}
The development of the spherical SOM has been driven by the border effects
observed in traditional SOM.  Two problems exist with the Spherical SOM. The
first is the level of control over the network size. The second is the
topologically induced errors caused by the arrangement of neurons on the sphere.
Both of these problems stem from the problem of uniformly distributing points on
a sphere. These problems will be investigated through the introduction of a new method for
testing topologically induced errors. The method first analyzes  the neural
network to find topological mis-matches, next we train the network with an
overwhelming about of synthetic data.  Through a series of simple plots we can
then compare each neurons internal variance as defined by the variance of the
observations that best fit that neuron with suchs metrics as neighborhood
influence, number of child observations, etc.
\end{abstract}


%\section{Problem Statement}
%1. Introduction (10\%)
%\\2. Background and Lit Review (30\%)
%\\	\ldots Problem Statement
%\\	\ldots Lit Review
%\\3. Research Design / Plan / Metodology (50\%)
%\\4. Significance and Limitations (10\%)
%\\5. Timetable

\section{Introduction}
Using a Spherical lattice is widely suggested as a solution to the boundary
effect in the traditional Self-Organizing Maps \citep{ritter99, boudjemai2003,
sangole03, Wu:2006lr, Nishio:2006fk}.  However, the use of the spherical lattice
introduces a new problem.  Save the five platonic solids, distributing points on
a sphere will allows result in irregular topology \citep{ritter99}.  The classic
method for generating a spherical lattice is to tessellate the sides a platonic
solid.  When tessellating the icosahedron, as described by \cite{Wu:2006lr}, the
resulting topology will always consist of 12 pentagons and \(N-12\) hexagons.
Where \(N\) is the total number of sides, or neurons.  The main drawback of this
method is the finite control over \(N\), which grows at a rate of \(f^2*10+2\),
where \(f\) is the frequency of the tessellation \citep{Wu:2006lr}.  The goal of
this research is to investigate the effects of irregular topology within
spherical SOM.
%Find a way to incorporate finite control over N into this sentence.  A question
%that has not yet been answered is to what degree does the irregulara topology
%effect the Spherical SOM.

\section{Background and Lit Review}
The Self-Organizing Map (SOM) is an unsupervised competitive learning process
developed by Teuvo Kohonen as a technique to analyze high dimensional data sets.
The SOM algorithum uses an artificial neural network to organize high
dimensional data onto a low dimensional lattice, or map, of neurons.  Each
neuron contains a reference vector that can be considered to model a portion of
the input space. Before training these neurons are initialized, most commonly to
random values.  During the training process a randomly selected input vector
searches the map for its best matching unit (BMU), that is the neuron to which
it is most similar. The BMU and it's neiborhood, as defined by a neighborhood
function, are than adjusted to better match that observation
\citep{Kohonen2000}.  The training process is repeated a predefined number of
times, or ideally until the map converges.  The traditional SOM is laid out on a
two dimensional plane using either a rectangular or hexagonal topology.
According to \cite{Wu:2006lr} the hexagonal structure is more uniform and
generally preferred.

\subsection{Boundary Effect}
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{gridedge_grey.pdf}
\caption{Fifty states plus D.C. mapped onto SOM trained with the first thirty-two census
variables.  Darker neurons have a relatively large differences from the mean of
the states, while lighter neurons are relatively closer.  Larger dots represent inputs that were poorly fit to the map, small smaller dots show inputs that were better bit to the map.}
\label{figure1}
\end{figure}

One obvious drawback of building the neural lattice in a discrete euclidean 
plane is the boundary of the resulting lattice.  A neuron located on the boundry has 
fewer neighbors and thus fewer chances of being updated \citep{Wu:2006lr}.  
As equivocally observed in Figure \ref{figure1}, neurons in the center of the 
map tend to better represent the mean of the input-space.  This is caused 
by outliers being pushed to the edges of the map, where they encounter fewer 
competing signals.

The toroidal som was introduced by \cite{li1993} and removes this drawback.
However, the torus is not hugely effective for visualization, as maps generated from 
a torus are not very intuitive \citep{ito2000,Wu:2006lr}.  \cite{ritter99} describes 
the torus as being topologically flat and suggests that a curved topology, such 
as that of a sphere, may better reflect direction data.  A sphere also results in a 
more intuitive map, since we are accustomed to looking at maps based on a sphere.

\subsection{Spherical SOM}
\cite{ritter99} first introduced the spherical SOM and several
enhancements have since been suggested 
\citep{Wu:2006lr,boudjemai2003,sangole03,Nishio:2006fk}.  A good
comparison of these enhancements can be found in \citep{Wu:2006lr}.  All of
these methods derive their spherical structure through the tessellation of a
polyhedron as originally proposed by \citeauthor{ritter99}.  \cite{Wu:2006lr}
point out the importance of a uniform distribution on the sphere and that it is
preferable for all neurons to have an equal number of neighbors and to be
equally spaced.  They find generally that the tessellation method best
satisfies this condition and specifically that the icosahedron is the best
starting point \citep{wu2005}.

\subsubsection{Network Size}
The literature offers little theoretical guidance on network size \citep{cho1996}. 
The SOM Toolbox documentation suggests simply using a network size 
of \(5*\sqrt {n}\), where \(n\) is the number of observations \citep{toolbox}.
The tessellation method used by the class of spherical SOM based on Ritter's
work results in a network size that is a function of the tessellation frequency
and therefore grows exponentially. In practice 2D Euclidean SOMs
also offer limited control over network size, as it is undesirable to have one
dimension dramatically larger then the other.  
\cite{Nishio:2006fk} try to address the issue of network size granularity by departing 
from the tessellation method and suggesting the use of a partitioned helix to uniformly distribute any
number of neurons on a sphere.  A similar method was dismissed by
\cite{wu2005} for failing to satisfy the uniformity condition.
\citeauthor{Nishio:2006fk} seem to have addressed part of the condition, but the
issue of having a uniform number of direct neighbors is still not addressed.

Tessellation of the icosahedron results in a network of neurons, each of which
have exactly six neighbors, save the original twelve which each have five.
This is beneficial, as the resulting maps will share many of the same
properties as maps generated using the traditional hexagonal topology in 2D
Euclidean space. The indexed data structure developed by
\citeauthor{Wu:2006lr} (GeoSOM) allows for fast identification of direct
(first order) and indirect (\textit{n\textsuperscript{th}} order) neighbors.
\cite{Nishio:2006fk} offer no such data structure nor any practical advice on
identification of neighbors; only variance in neuron spacing is addressed.

\subsection{Conclusion}
Methods, for distributing points on the sphere, which allow for fine grained 
control over network size produce slightly more irregular topologies.  However,
no description of these irregularities or their effects on SOM training has occurred in the
literature. Given that limited theoretical guidance is available for choosing network size the
desire for finer control over the network size, should not be overlooked. In
particular for a larger SOM the ideal network size may not be achievable via
tessellation of the icosahedron. In order to empirically investigate the effects of irregular 
topology in Spherical SOM a simple method is proposed in the next section.  The method
identifies topological irregularities in order to generate a single number representing a
given lattice's topologically induced errors.  It also allows us to visual the sites of those mis-matches.

%\section{Proposed Topology}
%The Rakhmanov algorithm has the ability to distribute any number of points
%onto the surface of a sphere, which allows for the finest possible level of
%control of network size, even greater than that of the traditional euclidean
%based SOM \citep{Rakhmanov94}.  There are two problems with the \citeauthor{Rakhmanov94}
%algorithm. The first is the variance in the number of direct neighbors for any
%given neuron.  The second is variance in the distance between those neurons.
%Using the algorithm to distribute 162 points, results in nearest neighbor
%``distances that vary from 0.15779 to 0.30069'' \cite[pg 3]{wu2005}.
%Furthermore 114 neurons have 6 direct neighbors, 30 have 5 and 18 have 7, as
%defined by their Delaunay Triangulation.  The fourth frequency tessellation of
%the icosahedron has nearest neighbor distances in the range of 0.25319 to
%0.31287, 150 neurons have 6 neighbors and 12 have 5.

%Irregularities in the spatial distribution of neurons exhibited by the Rakhmanov
%algorithm allows a given neuron's neighboring points to be easily ranked
%and sorted by their distance to the neuron in question.  The ranked and sorted
%neighbors can be further separated into distinct classes, or orders, such that
%the first order contains exactly \begin{math}1\times6\end{math} neurons and
%the \textit{n\textsuperscript{th}} order contains exactly
%\begin{math}n\times6\end{math} neurons.  Furthermore, neurons will be assigned
%a distance relative to the central neuron and equal to that of their order.
%For example, the six closest neurons will all be assigned a distance weight of
%unity, while the next twelve closest neurons will be assigned a distance
%weight of two and so on and so forth.  This follows from the properties of the
%standard hexagonal topology used traditionally in SOM and provides the
%foundation for a spherical SOM which mimic's that behavior.  This approach
%shall be referred to as HS-SOM.

\section{Methodology}
\subsection{Irregular Topology}
The basic problem of the "boundary effect" is that neurons on the edge have fewer neighbors. Yet there are only five possible arrangements of points on a sphere such that all points have the same number of neighbors.  Any spherical lattice consisting of more then twenty (dodecahedron) neurons will contain topological irregularities.  This is to say that not all neurons will have the same number of neighbors.  A simple test can be conducted to find the locations of these mis-matches.

\subsection{The Method}
Let, \begin{math}N\end{math}, be a set of neurons.  Each neuron will have an
associated one-dimensional reference vector with its sole value initialized to
zero. Each neuron \(n\) in the set \(N\) is trained with a constant input signal.  During the training
the input signal will be applied over \(n\)'s neighborhood as defined by a standard distance decay function.

\subsubsection{Demonstration}

\begin{figure}
\centering
\subfigure[a]{\includegraphics[width=.30\linewidth]{grid-a}}
\subfigure[b]{\includegraphics[width=.30\linewidth]{grid-bb}}
\subfigure[c]{\includegraphics[width=.30\linewidth]{grid-cb}}
\caption{The grids below are a simple example of a 3x3 neural lattice.  The first grid shows network after the top-right cell was trained with an input signal of two.  The second grid shows how the network looks after training one and two neurons respectivly. The final grid shows the complete training.  Try continuing where the second grid left off.  In the top-right cell mark II, and I in each of it's direct neighbors.  This }

\end{figure}

The proposed HS-SOM method raises questions of topological preservation.  
One can imagine a dense set of points evenly distributed on one hemisphere and a
sparse set of points distributed on the other. Neighborhoods near the division
would be greatly distorted; in terms of training, more updates would occur in
the dense set.  Given the overall uniformity produced by the
\citeauthor{Rakhmanov94} method this phenomenon should be minimized. However a
simple test can be conducted to check for it and further validate the HS-SOM
and other methods.

.  In the end, if true edgeless hexagonal topology is preserved all neurons should have the
same value. The mean squared deviation from each node's attribute vector to
the mean attribute vector may be used as a distortion measure comparable
across any SOM, both spherical and 2D of a given network size.

It is known that for a tessellated icosahedron distortions
will be observed at exactly twelve locations on the surface, around the twelve
original vertices.  For a hexagon lattice in 2D euclidean space the
distortions will be observed at the edges of the lattice.  It is hypothesized
that for HS-SOM, distortions will be less uniform in their spatial
distribution and their overall magnitude will be less then those exhibited by
either GeoSOM or SOM\_PAK.

Further the results can be visualized to show the exact location of
topologically caused errors on the lattice.  Since we know the distortions
will occur in GeoSOM, we can use these results to help form understanding and
possibly lead to an intuitive mathematical weighting scheme for GeoSOM such as
that proposed by \cite{Kohonen2000} for 2D SOM. The method allows us to
examine the suitability of the various algorithms for uniformly distributing
points on a sphere for use in HS-SOM.

For a benchmark, the experiment is carried out over two 3x3 grids, one of
hexagonal topology, the other of rectangular topology. The neighborhood
constant is set to unity; meaning only immediate (rook contiguity) neighbors
are included. The input vector's sole value shall equal two. After each
neuron was used once as a pseudo BMU (nine training cycles) the variance of
the rectangular grid was 0.4444, while the variance of the hexagonal grid was
1.5802. As shown in figures \ref{figure2} and \ref{figure3} the hexagonal lattice
experienced higher overall values.

It should be noted that neither the potential harm nor the possible benefits
of distortions have note been explored here.  It is feasible that distortions
may help the convergence process and eliminating them entirely could
unnecessarily prolong training.

\begin{figure}
\centering
\includegraphics[width=0.4\linewidth]{figure_hex.png}
\caption{Lower numbers and lighter colors indicate topological distortion.
In the case of this specific example eight is the ideal value for all neurons
in the hexagonal lattice, two for yourself plus one for each neighbor.  For
the rectangular lattice the ideal value is six.}
\label{figure2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.4\linewidth]{figure_rect.png}
\caption{Lower numbers and lighter colors indicate topological distortion.  In
the case of this specific example eight is the ideal value for all neurons in
the hexagonal lattice, two for yourself plus one for each neighbor.  For the
rectangular lattice the ideal value is six.}
\label{figure3}
\end{figure}

\section{Limitations}
The primary limitation of the HS-SOM method is speed in neighborhood
searching. Time complexity for neighborhood searching in HS-SOM is
\begin{math}O(N)\end{math}, while GeoSOM runs in
\begin{math}O(n)\end{math}\footnote{\textit{N} is the network size and
\textit{n} in the number of neurons in the current
neighborhood} \citep{Wu:2006lr}. Time and space complexity can be reduced by
caching neighborhood calcuations on-the-fly out to the current neighborhhod
size, as opposed to precalcuting neighborhoods out to initial neighborhood
size.
\section{Expected Results and Future Work}
\begin{itemize}
\item A new tool for measuring topological preservation will be introduced.
\item HS-SOM will prove to have less distortion then 2D euclidean SOM and hopefully GeoSOM.
\item Implement the validation method and test the GeoSOM and HS-SOM using the Rakhmanov and helix methods.
\item Develop more diagnostics, such as the reverse quantization error visualizations.
\end{itemize}



%\section{Bibliography}
\bibliographystyle{apalike}
\bibliography{som}
\end{document}
